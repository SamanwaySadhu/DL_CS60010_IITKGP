{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4 (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "nAmYWNSSa3u-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "0d9b5248-61e7-46eb-b7bc-2a453b84251f"
      },
      "cell_type": "code",
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.4.0.post0)\n",
            "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.14.6)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LeEcIWzMa69m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    },
    {
      "metadata": {
        "id": "_n7IBDNda9NX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import mxnet as mx\n",
        "from mxnet import nd, autograd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "mx.random.seed(1)\n",
        "ctx = mx.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AEn_vuTcbJ0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Preprocessing the data\n",
        "\n",
        "'''\n",
        "Read reviews from a JSON-formatted file into an array.\n",
        "'''\n",
        "lines = [] \n",
        "\n",
        "with open('train.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        lines.append(line)\n",
        "\n",
        "num_train = len(lines)\n",
        "\n",
        "with open('test.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        lines.append(line)        \n",
        "\n",
        "reviews = lines\n",
        "\n",
        "'''\n",
        "Clean each document by removing unnecesary characters and splitting by space.\n",
        "'''\n",
        "def clean_document(doco):\n",
        "    punctuation = string.punctuation + '\\n\\n';\n",
        "    punc_replace = ''.join([' ' for s in punctuation]);\n",
        "    doco_clean = doco.replace('-', ' ');\n",
        "    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n",
        "    trans_table = str.maketrans(punctuation, punc_replace);\n",
        "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')]);\n",
        "    doco_clean = doco_clean.split(' ');\n",
        "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0];\n",
        "    \n",
        "    return doco_clean;\n",
        "\n",
        "# Generate a cleaned reviews array from original review texts\n",
        "review_cleans = [clean_document(doc) for doc in reviews];\n",
        "sentences = [' '.join(r) for r in review_cleans]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPcmeEUkbYtR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lengths = [len(item.split(' ')) for item in sentences]\n",
        "MAX_SEQUENCE_LENGTH = max(lengths)\n",
        "\n",
        "word_list = []\n",
        "for sent in sentences:\n",
        "    words = sent.split(' ')\n",
        "    for word in words:\n",
        "        if word not in word_list:\n",
        "            word_list.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y4nUIDljdL9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Embeddings matrix using one-hot encoding.\n",
        "# EMBEDDING_DIM = 100\n",
        "one_hot_emb_matrix = nd.zeros((len(word_list) + 1, len(word_list) + 1), ctx = ctx)\n",
        "one_hot_emb_matrix[0,0] = 1\n",
        "for i in range(len(word_list)):\n",
        "    one_hot_emb_matrix[i+1,i+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsosTdoedOS8",
        "colab_type": "code",
        "outputId": "3f1e0af2-628e-43aa-d4b3-989163cb3b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "# rev_emb_index = {}\n",
        "!wget -O glove.6B.100d.txt https://worksheets.codalab.org/rest/bundles/0xd16b6c21f7a44270908b95992812f39f/contents/blob/\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "#     rev_emb_index[list(coefs)] = word\n",
        "f.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-24 16:58:51--  https://worksheets.codalab.org/rest/bundles/0xd16b6c21f7a44270908b95992812f39f/contents/blob/\n",
            "Resolving worksheets.codalab.org (worksheets.codalab.org)... 40.71.231.153\n",
            "Connecting to worksheets.codalab.org (worksheets.codalab.org)|40.71.231.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘glove.6B.100d.txt’\n",
            "\n",
            "glove.6B.100d.txt       [            <=>     ] 331.04M   137MB/s    in 2.4s    \n",
            "\n",
            "2019-03-24 16:58:54 (137 MB/s) - ‘glove.6B.100d.txt’ saved [347116733]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UD-8DuqsdSCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Embeddings matrix using pre-trained embeddings.\n",
        "EMBEDDING_DIM = 100\n",
        "pretr_emb_matrix = nd.zeros((len(word_list) + 1, EMBEDDING_DIM), ctx = ctx)\n",
        "for i, word in enumerate(word_list):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        pretr_emb_matrix[i + 1] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SmxHkL_Tdayt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_test_data(num_train, word_list, emb_matrix, sentences, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
        "    data = nd.zeros((len(sentences), MAX_SEQUENCE_LENGTH-1, EMBEDDING_DIM), ctx = ctx)\n",
        "    for idx, sent in enumerate(sentences):\n",
        "        words = sent.split(' ')[:-1]\n",
        "        for i in range(MAX_SEQUENCE_LENGTH - 1 - len(sent)):\n",
        "            words.append('<\\s>')\n",
        "        for idx2, word in enumerate(words):\n",
        "            if word not in word_list:\n",
        "                data[idx, idx2, :] = emb_matrix[0]\n",
        "            else:\n",
        "                data[idx, idx2, :] = emb_matrix[word_list.index(word)]\n",
        "    return(data[0:num_train], data[num_train:])\n",
        "#returns X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "95uhSDLJdeja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Using pre-trained embedding\n",
        "train, test = train_and_test_data(num_train, word_list, pretr_emb_matrix, sentences, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-vahI-_6dgyb",
        "colab_type": "code",
        "outputId": "cc929268-4128-4f7b-c4e9-adc7fdbc9269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "seq_length = MAX_SEQUENCE_LENGTH - 1\n",
        "vocab_size = len(word_list) + 1\n",
        "batch_size = 50\n",
        "num_batches = len(train) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "train_data = train[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, EMBEDDING_DIM))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "train_data = nd.swapaxes(train_data, 0, 1)\n",
        "train_data = nd.swapaxes(train_data, 1, 2)\n",
        "print(train_data.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of batches:  72\n",
            "(72, 17, 50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I124CAjKdjrx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_test_labels(num_train, word_list, emb_matrix, sentences, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
        "    data = nd.zeros((len(sentences), MAX_SEQUENCE_LENGTH-1, EMBEDDING_DIM), ctx = ctx)\n",
        "    for idx, sent in enumerate(sentences):\n",
        "        words = sent.split(' ')[1:]\n",
        "        for i in range(MAX_SEQUENCE_LENGTH - 1 - len(sent)):\n",
        "            words.append('<\\s>')\n",
        "        for idx2, word in enumerate(words):\n",
        "            if word not in word_list:\n",
        "                data[idx, idx2, :] = emb_matrix[0]\n",
        "            else:\n",
        "                data[idx, idx2, :] = emb_matrix[word_list.index(word)]\n",
        "    return(data[0:num_train], data[num_train:])\n",
        "#returns X_train, y_train, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soFJwTYwdoZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Using pre-trained embedding\n",
        "train_l, test_l = train_and_test_labels(num_train, word_list, pretr_emb_matrix, sentences, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsNITxUGdtDI",
        "colab_type": "code",
        "outputId": "5873f1bd-4145-4b98-e4bf-35a885afcefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "seq_length = MAX_SEQUENCE_LENGTH - 1\n",
        "vocab_size = len(word_list) + 1\n",
        "batch_size = 50\n",
        "num_batches = len(train_l) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "train_label = train_l[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, EMBEDDING_DIM))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "train_label = nd.swapaxes(train_label, 0, 1)\n",
        "train_label = nd.swapaxes(train_label, 1, 2)\n",
        "print(train_label.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of batches:  72\n",
            "(72, 17, 50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tr5QqME6dvsc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(y_linear, temperature=1.0):\n",
        "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
        "    exp = nd.exp(lin)\n",
        "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
        "    return exp / partition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BzEcgp02dyDR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_rnn(inputs, state, temperature=1.0):\n",
        "    outputs = []\n",
        "    h = state\n",
        "    for X in inputs:\n",
        "        h_linear = nd.dot(X, Wxh) + nd.dot(h, Whh) + bh\n",
        "        h = nd.tanh(h_linear)\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGaRWbETd0Xy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy(yhat, y):\n",
        "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9Pux2Std42Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def average_ce_loss(outputs, labels):\n",
        "    assert(len(outputs) == len(labels))\n",
        "    total_loss = 0.\n",
        "    for (output, label) in zip(outputs,labels):\n",
        "        total_loss = total_loss + cross_entropy(output, label)\n",
        "    return total_loss / len(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDKkCPZ0d9Lb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grad_clip(grad, max_grad):\n",
        "    if len(grad.shape) == 1:\n",
        "        for i in range(grad.shape[0]):\n",
        "            if grad[i] > max_grad:\n",
        "                grad[i] = max_grad\n",
        "            elif grad[i] < -max_grad:\n",
        "                grad[i] = -max_grad\n",
        "    elif len(grad.shape) == 2:\n",
        "        for i in range(grad.shape[0]):\n",
        "            for j in range(grad.shape[1]):\n",
        "                if grad[i][j] > max_grad:\n",
        "                    grad[i][j] = max_grad\n",
        "                elif grad[i][j] < -max_grad:\n",
        "                    grad[i][j] = -max_grad\n",
        "    return(grad)\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param.grad[0]=mx.ndarray.clip(param.grad[0],-10,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1RXffdUPeaYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_inputs = EMBEDDING_DIM\n",
        "num_hidden = 64\n",
        "num_outputs = EMBEDDING_DIM\n",
        "\n",
        "########################\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "########################\n",
        "Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=mx.cpu(0)) * .01\n",
        "\n",
        "########################\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "########################\n",
        "Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=mx.cpu(0)) * .01\n",
        "\n",
        "########################\n",
        "#  Bias vector for hidden layer\n",
        "########################\n",
        "bh = nd.random_normal(shape=num_hidden, ctx=mx.cpu(0)) * .01\n",
        "\n",
        "\n",
        "########################\n",
        "# Weights to the output nodes\n",
        "########################\n",
        "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=mx.cpu(0)) * .01\n",
        "by = nd.random_normal(shape=num_outputs, ctx=mx.cpu(0)) * .01\n",
        "\n",
        "# NOTE: to keep notation consistent,\n",
        "# we should really use capital letters\n",
        "# for hidden layers and outputs,\n",
        "# since we are doing batchwise computations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7FaPanDefBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = [Wxh, Whh, bh, Why, by]\n",
        "\n",
        "for param in params:\n",
        "    param.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjingt5cewml",
        "colab_type": "code",
        "outputId": "fcbdb9e2-52e0-46d0-a360-d8c74dcaf34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14734
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 12\n",
        "moving_loss = 0.\n",
        "\n",
        "learning_rate = .1\n",
        "\n",
        "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "for e in range(epochs):\n",
        "    ############################\n",
        "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
        "    ############################\n",
        "    if ((e+1) % 100 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=mx.cpu(0))\n",
        "    for i in range(num_batches):\n",
        "#         print(i)\n",
        "        dat_one_hot = train_data[i]\n",
        "        lab_one_hot = train_label[i]\n",
        "        data_one_hot = nd.zeros(dat_one_hot.shape, ctx=mx.cpu(0))\n",
        "        label_one_hot = nd.zeros(lab_one_hot.shape, ctx=mx.cpu(0))\n",
        "        dat_one_hot.copyto(data_one_hot)\n",
        "        lab_one_hot.copyto(label_one_hot)\n",
        "        with autograd.record():\n",
        "            outputs, state = simple_rnn(data_one_hot, state)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "        print(i)\n",
        "#         for param in params:\n",
        "#             print(param.grad)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = np.mean(loss.asnumpy()[0])\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
        "\n",
        "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 0. Loss: -3.300260478674844\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 1. Loss: -3.2284833421861565\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 2. Loss: -3.193672050406152\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 3. Loss: -3.176788874304754\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 4. Loss: -3.168600679592759\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 5. Loss: -3.164629475819317\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 6. Loss: -3.1627034762595945\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 7. Loss: -3.1617693830939797\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 8. Loss: -3.1613163559696233\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 9. Loss: -3.1610966417238116\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 10. Loss: -3.1609900822106654\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "Epoch 11. Loss: -3.1609384017663693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pXhQ8L-Ye03M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('Pretrained_embeddings_parameters.pkl','wb') as fp:\n",
        "    pickle.dump(params,fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "av9TY2bRj0UQ",
        "colab_type": "code",
        "outputId": "ec640fa3-79b0-4185-ddd3-c669e5c540f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "#inference\n",
        "seq_length = MAX_SEQUENCE_LENGTH - 1\n",
        "vocab_size = len(word_list) + 1\n",
        "batch_size = 50\n",
        "num_batches = len(test) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "test_data = test[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, EMBEDDING_DIM))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "test_data = nd.swapaxes(test_data, 0, 1)\n",
        "test_data = nd.swapaxes(test_data, 1, 2)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of batches:  13\n",
            "(13, 17, 50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EzE5WqAWj3Ht",
        "colab_type": "code",
        "outputId": "f6ffca96-9d3f-4fb1-eabb-eea7d333986f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "seq_length = MAX_SEQUENCE_LENGTH - 1\n",
        "vocab_size = len(word_list) + 1\n",
        "batch_size = 50\n",
        "num_batches = len(test_l) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "test_label = test_l[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, EMBEDDING_DIM))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "test_label = nd.swapaxes(test_label, 0, 1)\n",
        "test_label = nd.swapaxes(test_label, 1, 2)\n",
        "print(test_label.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of batches:  13\n",
            "(13, 17, 50, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVGQKNI7j5cO",
        "colab_type": "code",
        "outputId": "838e4761-bee3-4ad8-84e4-67dc07c1fb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Last word only\n",
        "state = nd.zeros(shape=(batch_size, num_hidden), ctx=mx.cpu(0))\n",
        "count=0\n",
        "# preds = []\n",
        "for i in range(num_batches):\n",
        "#         print(i)\n",
        "    dat_one_hot = test_data[i]\n",
        "    lab_one_hot = test_label[i]\n",
        "    data_one_hot = nd.zeros(dat_one_hot.shape, ctx=mx.cpu(0))\n",
        "    label_one_hot = nd.zeros(lab_one_hot.shape, ctx=mx.cpu(0))\n",
        "    dat_one_hot.copyto(data_one_hot)\n",
        "    lab_one_hot.copyto(label_one_hot)\n",
        "#         with autograd.record():\n",
        "    outputs, state = simple_rnn(data_one_hot, state)\n",
        "    pred = outputs[-1]\n",
        "    target = label_one_hot[-1,:,:]\n",
        "    for i in range(batch_size):\n",
        "        if sum(list(pred[i] - target[i])) == 1.0000:\n",
        "            count+=1\n",
        "print('Accuracy is ' + str(100*count/(batch_size*num_batches)) + str('%.'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 0.46153846153846156%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eq3cessZj84L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}